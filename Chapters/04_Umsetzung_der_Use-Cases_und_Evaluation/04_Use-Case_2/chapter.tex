\section{Use-Case 2: Road Warrior} \label{Use-Case 2: Road Warrior}
%Anforderung: Ende-zu-Ende-Erreichbarkeit! Für z.B. Endpoint Scanning
%Gleiches Profil für alle VPN-Konzentratoren

\subsection{Umsetzung: Kerntätigkeiten}

%GeoIP
%Terraform DNS Update
%Bind Split-DNS

In AWS und Azure wurden via Terraform die virtuellen Maschinen VyOS aus den jeweiligen Marketplaces installiert.
Es wurden für die Cloud-Standorte AWS und Azure analog zu Use-Case 1 Templates erstellt, um den Client-VPN-Konzentrator zu installieren. In der Terraform Ressource azurerm\_linux\_virtual\_machine wird das richtige VyOS-Image aus dem Cloud-Marketplace referenziert über source\_image\_reference und lässt dieses installieren. Einige dieser Markeplace-Images bieten verschiedene Support- und Lizenz-Verträge an. Über den \textit{plan \{\}}-Kontext wird der passende Contract ausgewählt (hier: \glqq Standard support\grqq{}). Ähnlich ist für die AWS-Instanz vorzugehen.

\begin{lstlisting}[label=tf-azure-vyos-machine-image,caption=Mit der Terraform-Ressource wird das passende VyOS-Image gesucht und installiert.]
resource "azurerm_linux_virtual_machine"  "azurerm_linux_virtual_machine" {
  name = "vyos-test"
  location = var.rg_location
  resource_group_name = var.rg_name
  admin_username = "vyos"
  size = "Standard_B1s"
  network_interface_ids = [ azurerm_network_interface.azurerm_network_interface.id ]
  os_disk {
    caching = "ReadWrite"
    storage_account_type = "Premium_LRS"
  }
  admin_ssh_key {
    username   = "vyos"
    public_key = file("~/terraform/.cred_mgmt/_push/authorized_keys/id_rsa_automation.pub")
  }
  source_image_reference {
    publisher = "sentriumsl"
    offer = "vyos-1-2-lts-on-azure"
    version = "latest"
    sku = "vyos-1-2-crux"
  }
  plan {
    name = "vyos-1-2-crux"
    product = "vyos-1-2-lts-on-azure"
    publisher = "sentriumsl"
  }
}
\end{lstlisting}

Konfiguriert wurden die virtuellen Maschinen wieder über ein Terraform-Template. Das Public/Private-Schlüsselpaar wurde vorab erstellt, von der pfSense-PKI signiert und mit dem Zertifikat auf dem Terraform-Server gespeichert. Während des Terraform-Deployments wird Private Key und Zertifikat per Terraform Provisioner auf das Zielsystem kopiert. Sie werden in der Config dann via set interfaces openvpn vtun0 tls cert-file / key-file referenziert. In der Praxis sollte darauf verzichtet werden, das Schlüsselmaterial \glqq auf einem anderen System vorzulagern \grqq{}: ein privater Schlüssel nur auf dem Server erzeugt und gespeichert werden, auf dem dieser final genutzt wird. Optimalerweise wäre dies zur Laufzeit der Terraform-Deployments. Zur Vereinfachung des Use-Cases wurde auf diesen Schritt verzichtet. Eine Möglichkeit zur automatisierten Erzeugung von vertrauenswürdigen Zertifikaten ist das Simple Certificate Enrollment Protocol (SCEP)\cite[S.554]{Schmeh2013}.

Für den sich verbindenden VPN-Client sind diverse Konfigurationen hinterlegt: Domain-Name, (interner) Name-Server und Routen, die über den VPN-Konzentrator erreicht werden können.
Weiterhin wird ein MSS-Clamping vorgenommen, um IP-Fragmentierung von TCP-Applikationen möglichst zu verhindern.\\
Das Subnetz, in dem die OpenVPN-Clients terminiert werden, wird wieder dynamisch aus dem IPAM alloziert (s. set interfaces openvpn vtun0 server subnet).

\begin{lstlisting}[label=vpn-c2s-server-config,caption=Terraform Template für VyOS OpenVPN-Server in AWS]
set interfaces openvpn vtun0 openvpn-option "--mssfix 1350"
set interfaces openvpn vtun0 server domain-name 'intern.ba.mungard.de'
set interfaces openvpn vtun0 server name-server '192.168.201.1'
#"dynamic" routes?
set interfaces openvpn vtun0 server push-route '10.32.0.0/16'
set interfaces openvpn vtun0 server push-route '10.33.0.0/16'
set interfaces openvpn vtun0 server push-route '192.168.201.0/24'
set interfaces openvpn vtun0 server subnet ${aws_vpn_clients_subnet}/${aws_vpn_clients_subnet_mask}
set interfaces openvpn vtun0 tls ca-cert-file '/config/auth/BA_CA_2021.crt'
set interfaces openvpn vtun0 tls cert-file '/config/auth/vpn-server-aws.ba.mungard.de.crt'
set interfaces openvpn vtun0 tls dh-file '/config/auth/dh2048.pem'
set interfaces openvpn vtun0 tls key-file '/config/auth/vpn-server-aws.ba.mungard.de.key'
\end{lstlisting}

Die drei Cloud-Standorte wurden in diesem Proof-of-Concept verteilt über verschiedenen Regionen: Dublin (Azure), Frankfurt (AWS), Kiel (Private Cloud). Es werden drei Roadwarrior-Clients simuliert, die ebenso am jeweiligen Standort stehen. Dazu wurden weitere VMs an den Cloud-Standorten hochgefahren: Sie wurden völlig isoliert von dem restlichen Deployment installiert und sich nicht Teil der internen Infrastruktur. Zugang zu internen Ressourcen wird nur über eine erfolgreiche Authentifizierung am VPN-Konzentrator gewährleistet.

\begin{lstlisting}[label=ext-ip-addr-roadwarrior,caption=Der simulierte Roadwarrior-Client ist nicht Teil der Netzwerke 10.32.0.0/16 bzw. 10.33.0.0/16]
ubuntu@vpn-client-azure:~$ ip -brief -4 address show | grep -v lo
eth0             UP             10.42.0.4/24
\end{lstlisting}

Internet-Traffic ist möglich, Zugriff auf interne Ressourcen ist nicht möglich: 192.168.201.1 ist eine IP-Adresse der Private Cloud.

\begin{lstlisting}[label=internet-access-roadwarrior,caption=Der Roadwarrior-Client kann auf das Internet zugreifen - kein Zugriff auf interne Ressourcen.]
#Internet-Traffic
ubuntu@vpn-client-azure:~$ ping -c 1 8.8.8.8
PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.
64 bytes from 8.8.8.8: icmp_seq=1 ttl=112 time=0.987 ms

--- 8.8.8.8 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.987/0.987/0.987/0.000 ms

#Interne Adresse ist nicht erreichbar 
ubuntu@vpn-client-azure:~$ ping -c 1 192.168.201.1
PING 192.168.201.1 (192.168.201.1) 56(84) bytes of data.

--- 192.168.201.1 ping statistics ---
1 packets transmitted, 0 received, 100% packet loss, time 0ms
\end{lstlisting}

Die MaxMind GeoIP-Datenbank wurde auf dem DNS-Server installiert und in der Bind-Konfiguration referenziert:

\begin{lstlisting}[label=bind-geoip-directory,caption=.]
$ grep -E 'options|geoip-directory|^};' < named.conf.options
options {
        geoip-directory "/usr/share/GeoIP";
};
\end{lstlisting}

Dadurch wurde ermöglicht, GeoIP-Daten in einer Access Control List (ACL) zu nutzen und verschiedene Fälle zu definieren. Die ACL heißt in diesem Falle \glqq dublin\grqq{} und erlaubt Zugriffe aus der Stadt Dublin bzw. Zugriff für den TSIG-Key db-vpn.ba.mungard.de. Dieser Key wird benötigt, um Zonen-Updates machen zu können und ist im entsprechenden Terraform-Modul hinterlegt.

\begin{lstlisting}[label=acl-geoip-bind,caption=.]
acl "dublin" {
        geoip city Dublin;
        key "db-vpn.ba.mungard.de.";
};
\end{lstlisting}

Weiterhin existiert ein \textit{View} \glqq Dublin\grqq{}, in dem die ACL und Zonen-Daten zusammengeführt werden. Mit \textit{allow-update} werden Zonen-Updates explizit und ausschließlich für genannten TSIG-Key erlaubt.

\begin{lstlisting}[label=view-geoip-bind,caption=.]
view "dublin" {
        match-clients { dublin; };
        zone "ba.mungard.de" {
                type master;
                file "/etc/bind/ba_zones/db.geo.ba.mungard.de-dublin";
                allow-update { key "db-vpn.ba.mungard.de."; };
        };
};
\end{lstlisting}

In der Zonen-Datei für Dublin befindet sich vor Terraform-Deployment nur ein A-Record \textit{default-vpn}. Dies ist der A-Record, der als Fallback dient. Gemäß Evaluationskriterien muss ein Roadwarrior-Client auf die Private Cloud zurückfallen, wenn bisher kein Deployment stattgefunden hat. Die öffentlich auflösbare Zone lautet \textit{ba.mungard.de}.

\begin{lstlisting}[label=zone-dublin-before-deployment,caption=Die Zone ba.mungard.de vor dem Terraform-Deployment.]
$ grep -A1 "ORIGIN ba.mungard.de" < /etc/bind/ba_zones/db.geo.ba.mungard.de-dublin
$ORIGIN ba.mungard.de.
default-vpn             A       195.244.254.197
\end{lstlisting}

Die Konfigurationen für Frankfurter und Kieler Clients wurden analog vollzogen. Dies gilt ebenso für das interne DNS. Allerdings wird hier kein GeoIP genutzt, sondern anhand der Absender-IP-Adresse wird eine ACL (hier \glqq dublin-internal\grqq{}) \textit{getroffen}. Auch die internen Zonen sind durch Terraform via DNS UPDATE editierbar (key \glqq db-vpn-internal.ba.mungard.de.\grqq{}).

\begin{lstlisting}[label=view-internal-bind,caption=Absender-IPs aus dem Netzwerk 10.32.0.0/16 treffen die ACL dublin-internal.]
acl "dublin-internal" {
        10.32.0.0/16;
        key "db-vpn-internal.ba.mungard.de.";
};
\end{lstlisting}

Weiterhin gibt es eine ACL \glqq match-all\grqq{}, welche für anfragende Clients vorgesehen ist, die nicht per GeoIP identifiziert werden. Durch das Netzwerk 0.0.0.0/0 treffen die ACL alle restlichen Teilnehmer des Internets.

\begin{lstlisting}[label=view-matchall-bind,caption=.]
acl "match-all" {
        0.0.0.0/0;
        key "all-vpn.ba.mungard.de.";
};
\end{lstlisting}


Aus Sicht des Terraform-Moduls muss erstens ein Provider \textit{pro} TSIG-Key konfiguriert werden, weiterhin. Der server-Parameter konfiguriert den Nameserver, der per DNS UPDATE editiert werden soll. Außerdem ist das Pre-Shared-Secret (TSIG-Key) hinterlegt. Es wird für die Domain vpn.ba.mungard.de die IP-Adresse des VPN-Konzentrators in Dublin (Azure) eingetragen, welche dynamisch zur Laufzeit entsteht (\textit{var.dublin\_ip}).

\begin{lstlisting}[label=tf-provider-dns,caption=.]
provider "dns" {
        alias = "dns-dublin"
        update {
                server        = "192.168.200.29"
                key_name      = "db-vpn.ba.mungard.de."
                key_algorithm = "hmac-sha256"
                //Key fuer die Dokumentation aus Sicherheitsgruenden geaendert
                key_secret    = "9jzMaHwILwTzx9b/Ws8RAfBwC+eJpjOI2hEmAgCjV/4="
        }
}
resource "dns_a_record_set" "vpn-dublin" {
        provider = dns.dns-dublin
        zone = "ba.mungard.de."
        name = "vpn"
        addresses = [ var.dublin_ip ]
        ttl = 15
}
\end{lstlisting}

Nach dem Terraform-Deployment kann man sich per \textit{rndc dumpdb -zones} alle Views ausgeben lassen (hier eingefiltert auf die relevantesten Einträge der Views \glqq dublin\grqq{} und \glqq match-all\grqq{}). Man sieht, dass in beiden Views für die Domain default-vpn.ba.mungard.de die gleiche IP (\glqq Fallback\grqq{}) zurückgegegeben. Für Domain vpn.ba.mungard.de wird ist im View \glqq dublin\grqq{} nur eine Azure-IP sichtbar, während im View \glqq match-all\grqq{} IPs aller VPN-Konzentratoren des Deployments auftreten.

\begin{lstlisting}[label=tf-provider-dns,caption=.]
$ grep -E -A12 'Start view (dublin|match-all)$' < /var/cache/bind/named_dump.db | grep -Ev '(client|SOA|NS|nu|^;$)'
; Start view dublin
; Zone dump of 'ba.mungard.de/IN/dublin'
default-vpn.ba.mungard.de.                    15 IN A           195.244.254.197
vpn.ba.mungard.de.                            15 IN A           137.116.237.246
; Start view nu
--
; Start view match-all
; Zone dump of 'ba.mungard.de/IN/match-all'
default-vpn.ba.mungard.de.                    15 IN A           195.244.254.197
vpn.ba.mungard.de.                            15 IN A           18.197.9.208
vpn.ba.mungard.de.                            15 IN A           137.116.237.246
vpn.ba.mungard.de.                            15 IN A           195.244.254.197
\end{lstlisting}

Zusammenfassend die GeoIP-Konfiguration als Entscheidungsbaum.
\begin{figure}[h]
  \centering
  \includegraphics{Figures/entscheidungsbaum_bind_geoip.pdf}
  \caption{Entscheidungsbaum: GeoIP}
  \label{grafik:Use-Case_2_Entscheidungsbaum_GeoIP}
\end{figure}\FloatBarrier

Weiterhin ein exemplarischer Zone-Dump für einen internen Client, welcher über Dublin verbunden ist. Die interne Zone ist intern.ba.mungard.de. Alle Cloud-Standorte nutzen den gleichen Name-Server (ns1.intern.ba.mungard.de), welcher in der Private Cloud vorhanden ist.\\

\begin{lstlisting}[label=tf-provider-dns,caption=.]
$ grep -E -A10 'Start view dublin-internal' < /var/cache/bind/named_dump.db | grep -Ev '(client|SOA|NS|^;$)'
; Start view dublin-internal
; Zone dump of 'intern.ba.mungard.de/IN/dublin-internal'
ns1.intern.ba.mungard.de.                     15 IN A           192.168.201.1
www.intern.ba.mungard.de.                     15 IN A           10.32.0.4
\end{lstlisting}

%Deployment Webserver
Es wurde eine Ubuntu-VM pro Standort (hier Azure in Dublin) via Terraform deployed. Per remote-exec provisioner wird das Apache2-Paket installiert und die Default-HTML-Seite (\glqq It works!\grqq{}) per \textit{sed} manipuliert. Damit soll der Standort verdeutlicht werden, von dem die HTML-Seite ausgeliefert wurde. Analoge Konfiguration hat für den Standort AWS in Frankfurt stattgefunden.

\begin{lstlisting}[label=tf-provider-dns,caption=.]
resource "null_resource" "www_config" {
        depends_on = [ azurerm_virtual_machine.azurerm_virtual_machine ]
        connection {
                type = "ssh"
                host = azurerm_network_interface.azurerm_network_interface.private_ip_address
                user = "ubuntu"
                private_key = file(var.private_key_file)
        }

        provisioner "remote-exec" {
                inline = [ "sudo http_proxy=\"http://192.168.201.1:8080\" apt update",
                           "sudo http_proxy=\"http://192.168.201.1:8080\" apt install -y apache2",
                           "sudo sed -i 's/It works!/It works! @Azure Cloud (Location: \"North Europe - Dublin\")/' /var/www/html/index.html" ]
        }
}
\end{lstlisting}

% Binärer Entscheidungsbaum OpenVPN

Die OpenVPN Client-Konfigurationen wurden vor Terraform-Deployment bereits \textit{inline} in einer Textdatei vorbereitet. Auch hier gilt analog zur Server-Konfiguration, dass Schlüsselmaterial optimalerweise erst zur Laufzeit erstellt wird. Zertifikate und privater Schlüssel wurden im folgenden Beispiel gefiltert. Von zentraler Bedeutung sind Zeilen 2 und 3: Durch mehrfache Angabe einer remote-Direktive kann ein Fallback auf default-vpn.ba.mungard.de stattfinden, sollte vpn.ba.mungard.de nicht auflösbar sein.\\
Dies gilt für den Fall, falls das Deployment noch nicht stattgefunden hat und der Client nicht in der Nähe von Kiel ist, um per GeoIP-ACL Zugriff auf die Auflösung von vpn.ba.mungard.de erhält. Per pullfilter ignore redirect-gateway wird verhindert, dass ein Default Gateway über den Tunnel signalisiert wird: Alle weiteren, bekannten Routen kommen über die OpenVPN-Server-Konfiguration (s. oben).

\begin{lstlisting}[label=ovpn-client-conf,caption=.]
$ grep -B100 '<ca>' ~/terraform/.cred_mgmt/_push/openvpn/azure_client/config.ovpn | grep -v '<ca>'
remote vpn.ba.mungard.de 61231
remote default-vpn.ba.mungard.de 61231
port 0
pull-filter ignore redirect-gateway
cipher aes-256-cbc
client
dev tun
resolv-retry infinite
remote-cert-tls server
verb 3
persist-tun
\end{lstlisting}

Dieser simple Entscheidungsbaum soll den wichtigen Aspekt des \glqq DNS-Fallbacks\grqq{} nochmals verdeutlichen.

\begin{figure}[h]
  \centering
  \includegraphics{Figures/entscheidungsbaum_openvpn_config.pdf}
  \caption{Entscheidungsbaum: OpenVPN-Verbindungsversuch}
  \label{grafik:Use-Case_2_Entscheidungsbaum_OpenVPN}
\end{figure}\FloatBarrier

\subsection{Probleme und Lösungsfindung}
%VyOS Images finden? az CLI, AWS Suche funktioniert nicht
%Die richtigen Images für die VyOS-Maschinen der jeweiligen Cloud-Marketplaces auf Terraform zu übertragen, erwies sich als nicht trivial...
%Lizenzen muss zugestimmt werden -> manuell!!!!

%Statische Routen innerhalb VPC nicht more specific -> Transit Gateway
%Import von Remote State, um Azure VPN Gateway zu verkürzen... Eigentlich ziemlich coole Idee von mir... :)
%mmdb.py damit man nicht "blind" ist...

\textbf{\underline{Beschleunigung des Deployments}}\\
Wie bereits in Use-Case 1 (Lösungsfindung...) geschildert, benötigt das Azure VPN Gateway viel Zeit für die Erstellung. Da Use-Case 2 direkt auf diesem Use-Case aufsetzt, hätte das ebenso eine lange Deployment-Zeit nach sich gezogen. So war ursprünglich war die Idee, den Programmcode aus Use-Case 1 zu klonen und um die Infrastruktur-Komponenten aus Use-Case 2 zu ergänzen. Somit hätte sich die Deployment Zeit $t_{gesamt}$ aus den jeweiligen Deployment-Zeiten der Infrastrukturen aus Use-Case 1 und Use-Case 2 zusammengesetzt:\\
$t_{gesamt} = t_{Infrastruktur_{U1}} + t_{Infrastruktur_{U2}}$

Somit kam die Frage auf, ob man das Deployment beschleunigen könnte. Da in Use-Case 2 viele Tests und Änderungen gemacht wurden, musste oftmals komplett re-deployet werden, Deployment-Zeiten von bis zu anderthalb Stunden (s. Probleme Redeployment) mit sich bringen kann.\\
Dieses Problem konnte mit terraform\_remote\_state zumindest abgemildert werden. So wurde Use-Case 1 als vorhanden angenommen und die Infrastruktur aus Use-Case 2 lediglich hinzugefügt. Der \textit{state import} findet in der Datei main.tf von Use-Case 2 statt und referenziert die Datei terraform.tfstate aus Use-Case 1.

\begin{lstlisting}[label=tf-remote-state-import,caption=.]
$ grep -A6 \"terraform_remote_state\" < main.tf
data "terraform_remote_state" "base_deployment" {
  backend = "local"
  config = {
    path = "../use_case_1/terraform.tfstate"
  }
}
\end{lstlisting}

Der Remote-State-Import dauert einen kurzen Moment, wird aber in der folgenden Betrachtung vernachlässigt.
Unter der Annahme, dass Use-Case 1 bereits deployet wurde, ist die Laufzeit des Deployments effektiv:\\
$t_{gesamt} = t_{Infrastruktur_{U2}}$

Im Allgemeinen hat sich dieser \glqq Teile-und-herrsche\grqq{}-Ansatz im Zusammenspiel mit Terraform als sehr hilfreich erwiesen, um Konsistenz-Probleme der asynchronen Cloud-APIs während der Deployment-Phase zu verringern \cite[S.183-184]{Brikman2019}. Probleme während des Deployments entstanden v.a. dann, wenn \glqq zu viele\grqq{} Abhängigkeiten der Infrastruktur-Komponenten im Spiel waren. Für die Verschaltung von \textit{gesplitteten} Terraform-Deployments eignet sich das Werkzeug Terragrunt \cite[S.98]{Brikman2019}.

%https://web.archive.org/web/20210123233358/https://docs.aws.amazon.com/vpc/latest/userguide/vpc-ug.pdf Seite 230
%Warum brauchen wir more specifics?
%CIDR / Subnet / Route Tables in Azure und AWS in technische Grundlagen
\textbf{\underline{More-Specific Route nicht möglich in VPC Routing-Tabelle}}\\
Die Roadwarrior Clients müssen in einem Subnetz \glqq hinter\grqq{} dem VPN-Konzentrator terminiert werden. Dafür werden Routing-Einträge notwendig, damit AWS VPC bzw. Azure VNET die Information besitzen, über welchen \textit{Next-Hop} das Client-VPN-Subnetz erreicht werden kann. Durch \textit{Route Tables} stellt dies auch keine Hürde dar in Azure. AWS hingegen bietet keine Möglichkeit, Subnetze mit einer längeren Netzmaske in der Route Table zu hinterlegen, falls sie ein Subnetz des VPC CIDR darstellen würden. Man spricht auch von \glqq more specifics\grqq{}.\\
Im folgenden Beispiel sei das VPC CIDR 192.168.0.0/16 (Target: local). Es soll eine Route auf ein Netzwerk 192.168.100.0/24 über die AWS Instance \glqq i-06230ccaa[...]\grqq{} (Next-Hop) gesetzt werden, was eine more specific Route zum VPC CIDR darstellt.
Folgende Fehlermeldung meldet das Amazon Dashboard beim Versuch, diese Route anzulegen:

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.4]{Figures/more_specific_not_allowed_aws.PNG}
  \caption{BLUB}
  \label{grafik:more_specific_not_allowed_aws}
\end{figure}\FloatBarrier

Präfixe zu nutzen, welche nicht more specific sind, ginge insofern, dass das Routing \textit{innerhalb} des VPCs möglich wird, allerdings werden diese Präfixe nicht durch BGP den anderen Stellen bekannt gemacht. Somit erreichen Pakete aus Netzwerken, welche nicht im AWS VPC \glqq leben{grqq}, das VPN-Client-Netzwerk nicht.\\
Hide-NAT ist, wie bereits beschrieben (s.o.), keine Lösung, da dies die Anforderung nach Ende-zu-Ende-Konnektivität verletzen würden.\\
Nach diversen Versuchen und Überlegungen konnte durch das AWS Transit Gateway dieses Problem erfolgreich gelöst werden. Es bietet die Möglichkeit \textit{Peerings} zwischen verschiedenen VPCs herzustellen. Die Idee war, zwei VPCs zu nutzen: ein VPC für Client-VPN und ein VPC für den (Web-)Server. Alle VPN-Verbindungen hängen fortan auf dem Transit Gateway, welches sowohl alle AWS-Routen kennt (statische Routing-Einträge) als auch alle weiteren Routen über BGP lernt. Die VPCs routen alle Ziele zu RFC1918-Netzwerken statisch auf das Transit Gateway, welches Pakete entweder weiter an das benachbarte VPC oder über das Backbone-Netzwerk zum Ziel weiterschickt. Das folgende Netzwerkdiagramm mit anhängigen Routing-Tabellen soll diesen Sachverhalt verdeutlichen. VPC CIDR und VPC Subnet werden in diesem Beispiel synonym verwendet, was formell nicht korrekt ist: Route Tables werden in Subnets genutzt, nicht im CIDR. Dies dient jedoch zur Vereinfach des Schaubildes (s. CIDR / Subnet / Route Table technische Grundlagen). Das Transit Gateway lernt die VPC CIDR automatisch, insofern Route Propagation eingeschaltet ist:

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.75]{Figures/Use-Case-2_AWS_TGW_Route-Tables.pdf}
  \caption{BLUB}
  \label{grafik:aws_tgw_route_tables}
\end{figure}\FloatBarrier

%NAT... aber keine Ende-zu-Ende...

\textbf{\underline{Untersuchung von GeoIP-Daten}}\\
%https://linux.die.net/man/1/file
Die GeoIP-Datenbank liegt als Binärformat auf der Festplatte.

\begin{lstlisting}[label=tf-remote-state-import,caption=.]
$ file /usr/share/GeoIP/GeoLite2-City.mmdb
/usr/share/GeoIP/GeoLite2-City.mmdb: data
\end{lstlisting}

%https://web.archive.org/web/20201110053320/https://github.com/maxmind/MaxMind-DB-Reader-python
%https://bind9.readthedocs.io/en/latest/security.html?highlight=geoip#access-control-lists
Die Datensätze mussten \glqq sichtbar\grqq{} gemacht werden, um eine Idee zu bekommen, welche Spezifizierer genutzt werden können, um die DNS-Auflösung in regionaler Abhängigkeit umzusetzen. Weiterhin war nicht sichergestellt, welche Qualität die Datensätze aufweisen. Um dies zu testen, wurde ein kurzes Python-Skript \textit{mmdb.py} geschrieben und das Modul \textit{maxminddb} des GeoIP-Anbieters MaxMind eingebunden. Per \textit{pretty print} konnten die entsprechenden Datensätze für eine IP-Adresse ausgegeben werden.

\begin{lstlisting}[label=mmdb-code-example,caption=.]
#!/usr/bin/python3
import maxminddb
import sys
import pprint
#aehnliche Code-Teile uebersprungen
print("++++++++++++++++++++ CITY INFO +++++++++++++++++++\n")
with maxminddb.open_database('/usr/share/GeoIP/GeoLite2-City.mmdb') as reader:
    pprint.pprint(reader.get(sys.argv[1]))
print("++++++++++++++++++++ CITY INFO +++++++++++++++++++\n")
\end{lstlisting}

Anbei eine Beispielausgabe für eine IP-Adresse der Fachhochschule Kiel.

\begin{lstlisting}[label=mmdb-show-ip,caption=.]
$ dig +short fh-kiel.de A
149.222.20.63

\

$ mmdb.py 149.222.20.63
++++++++++++++++++++ ASN INFO ++++++++++++++++++++

{'autonomous_system_number': 680,
 'autonomous_system_organization': 'Verein zur Foerderung eines Deutschen '
                                   'Forschungsnetzes e.V.'}
++++++++++++++++++++ ASN INFO ++++++++++++++++++++
[...]
++++++++++++++++++++ CITY INFO +++++++++++++++++++

{'city': {'geoname_id': 2891122,
          'names': {'de': 'Kiel',
                    'en': 'Kiel',
                    [...]
[...]
 'location': {'accuracy_radius': 20,
              'latitude': 54.3258,
              'longitude': 10.1379,
              'time_zone': 'Europe/Berlin'},
 'postal': {'code': '24149'},
[...]
++++++++++++++++++++ CITY INFO +++++++++++++++++++
\end{lstlisting}

Insgesamt erwiesen sich die Datensätze als sehr brauchbar und akkurat. Per cron-Job wird die Datenbank wöchentlich aktualisiert.

\subsection{Evaluation}

%Nachdem das Backbone-Netz aus Use-Case 1 um diverse Clients und Server erweitert wurde

Wie bereits beschrieben, wurde pro Cloud-Standort eine virtuelle Maschine hochgefahren, die einen Roadwarrior-Client simulieren soll. Diese Clients haben \textbf{keinen} Zugriff auf interne Komponenten, sondern sind lediglich mit dem Internet verbunden, über welches VPN-Verbindungen zum jeweiligen VPN-Konzentrator aufgebaut werden können.

Nach dem Deployment von Use-Case 2 wird geprüft, ob sich die Domain vpn.ba.mungard.de auflösen lässt. Dies passiert in der Azure-Cloud.

\begin{lstlisting}[label=dns-resolution-client-vpn,caption=.]
ubuntu@vpn-client-azure:~$ dig +short vpn.ba.mungard.de A
94.245.93.144
\end{lstlisting}

Die IP-Adresse 94.2.45.93.144 gehört Microsoft.

\begin{lstlisting}[label=whois-microsoft-ip,caption=.]
$ whois 94.245.93.144 | grep mnt | uniq
mnt-by:         MICROSOFT-MAINT
\end{lstlisting}

Das Ziel ist ca. 1 Millisekunde entfernt. Für diesen Test wurde ICMP eingehend manuell erlaubt in der Network Security Group, im Default Deployment erfolgt für ICMP Echo Request ein \textit{Dropping}:
\begin{lstlisting}[label=ping-microsoft-ip,caption=.]
ubuntu@vpn-client-azure:~$ ping 94.245.93.144
PING 94.245.93.144 (94.245.93.144) 56(84) bytes of data.
64 bytes from 94.245.93.144: icmp_seq=1 ttl=57 time=1.21 ms
64 bytes from 94.245.93.144: icmp_seq=2 ttl=57 time=0.936 ms
64 bytes from 94.245.93.144: icmp_seq=3 ttl=57 time=1.10 ms
64 bytes from 94.245.93.144: icmp_seq=4 ttl=57 time=1.04 ms
^C
--- 94.245.93.144 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3003ms
rtt min/avg/max/mdev = 0.936/1.073/1.216/0.106 ms
\end{lstlisting}

Die Annahme von nun an ist, dass die lokale DNS-Auflösung auch für AWS funktioniert. Daher folgt an dieser Stelle nur ein Ping mit vorheriger Auflösung des Hostnamens vpn.ba.mungard.de. Die RTT ist vergleichbar mit Azure (s.o.):

\begin{lstlisting}[label=ping-aws-ip,caption=.]
ubuntu@vpn-client-aws:~$ ping vpn.ba.mungard.de
PING vpn.ba.mungard.de (3.67.38.135) 56(84) bytes of data.
64 bytes from ec2-3-67-38-135.eu-central-1.compute.amazonaws.com (3.67.38.135): icmp_seq=1 ttl=63 time=0.930 ms
64 bytes from ec2-3-67-38-135.eu-central-1.compute.amazonaws.com (3.67.38.135): icmp_seq=2 ttl=63 time=1.16 ms
64 bytes from ec2-3-67-38-135.eu-central-1.compute.amazonaws.com (3.67.38.135): icmp_seq=3 ttl=63 time=0.970 ms
64 bytes from ec2-3-67-38-135.eu-central-1.compute.amazonaws.com (3.67.38.135): icmp_seq=4 ttl=63 time=0.947 ms
^C
--- vpn.ba.mungard.de ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3004ms
rtt min/avg/max/mdev = 0.930/1.001/1.157/0.091 ms
\end{lstlisting}

%https://www.hetzner.com/cloud
Weiterhin wurde eine virtuelle Maschine in der Hetzner-Cloud am Standort Nürnberg deployed, welche einen Roadwarrior-Client simuliert, welcher an keinem der Standorte verfügbar ist und somit die \glqq match-all\grqq-ACl trifft. Das dig-Kommando liefert drei A-Records für die Domain vpn.ba.mungard.de aus.

\begin{lstlisting}[label=dig-matchall-ip,caption=.]
root@openvpn-client:~# dig +short vpn.ba.mungard.de A
3.67.38.135
195.244.254.197
94.245.93.144
\end{lstlisting}

%normal
Weiter wurde getestet, ob sich ein VPN-Client mit dem lokalen VPN-Konzentrator verbindet, hier wieder Azure:
Der Filterausdruck ist für IPv4-Adressen und macht das Verbindungs-Log übersichtlicher.
%begin{lstlisting}[label=connection-to-vpn-normal,caption=.]
\begin{minted}[breaklines,frame=single]{text}
ubuntu@vpn-client-azure:~$ sudo openvpn --config config.ovpn 2>&1 | grep -E '(([0-9]{,3})\.){3}[0-9]{,3}'
Wed May 12 11:27:26 2021 TCP/UDP: Preserving recently used remote address: [AF_INET]94.245.93.144:61231
Wed May 12 11:27:26 2021 UDP link remote: [AF_INET]94.245.93.144:61231
Wed May 12 11:27:26 2021 TLS: Initial packet from [AF_INET]94.245.93.144:61231, sid=edbf2413 4ce1aa1c
Wed May 12 11:27:26 2021 [vpn-server-azure.ba.mungard.de] Peer Connection Initiated with [AF_INET]94.245.93.144:61231
Wed May 12 11:27:27 2021 PUSH: Received control message: 'PUSH_REPLY,dhcp-option DNS 192.168.201.1,route 10.32.0.0 255.255.0.0,route 10.33.0.0 255.255.0.0,route 192.168.201.0 255.255.255.0,dhcp-option DOMAIN intern.ba.mungard.de,route-gateway 10.32.1.1,topology subnet,ping 10,ping-restart 60,ifconfig 10.32.1.2 255.255.255.0'
Wed May 12 11:27:27 2021 ROUTE_GATEWAY 10.42.0.1/255.255.255.0 IFACE=eth0 HWADDR=00:0d:3a:dc:ec:4f
Wed May 12 11:27:27 2021 /sbin/ip addr add dev tun0 10.32.1.2/24 broadcast 10.32.1.255
Wed May 12 11:27:27 2021 /etc/openvpn/update-systemd-resolved tun0 1500 1557 10.32.1.2 255.255.255.0 init
May 12 11:27:27 update-systemd-resolved: Adding IPv4 DNS Server 192.168.201.1
Wed May 12 11:27:27 2021 /sbin/ip route add 10.32.0.0/16 via 10.32.1.1
Wed May 12 11:27:27 2021 /sbin/ip route add 10.33.0.0/16 via 10.32.1.1
Wed May 12 11:27:27 2021 /sbin/ip route add 192.168.201.0/24 via 10.32.1.1
\end{minted}
%end{lstlisting}

Um den Fallback zu testen, wurde eine spezielle iptables-Regel eingerichtet, die ausgehende Pakete, in denen der String \glqq vpn\grqq{} (für vpn.ba.mungard.de - DNS ist Klartext!) enthalten ist, verwirft. vpn-default ist hingegen erlaubt: die Regel wird über der erstgenannten Regel ausgewertet.

%begin{lstlisting}[label=iptables-drop-dns,caption=.]
\begin{minted}[breaklines,frame=single]{bash}
$ sudo iptables-save | grep vpn
-A OUTPUT -p udp -m string --string "default-vpn" --algo bm --to 65535 -m udp --dport 53 -j ACCEPT
-A OUTPUT -p udp -m string --string "vpn" --algo bm --to 65535 -m udp --dport 53 -j DROP
\end{minted}
%end{lstlisting}

Die Domain vpn.ba.mungard.de ist daraufhin nicht mehr auflösbar, default-vpn.ba.mungard.de hingegen schon.

\begin{lstlisting}[label=connection-fail-to-vpn-fallback-to-default-vpn,caption=.]
ubuntu@vpn-client-azure:~$ dig +short vpn.ba.mungard.de A
;; connection timed out; no servers could be reached

ubuntu@vpn-client-azure:~$ dig +short default-vpn.ba.mungard.de A
195.244.254.197
\end{lstlisting}

Dies macht sich auch beim Verbindungsversuch mit OpenVPN bemerkbar. Ein Fallback auf die IP 195.244.254.197 erfolgt, welche der Domain default-vpn.ba.mungard.de entspricht (s.o.).
\begin{lstlisting}[label=connection-fail-vpn-fallback-to-default-vpn,caption=.]
ubuntu@vpn-client-azure:~$ sudo openvpn --config config.ovpn 2>&1
Wed May 12 12:19:14 2021 RESOLVE: Cannot resolve host address: vpn.ba.mungard.de:61231 (Name or service not known)
Wed May 12 12:19:14 2021 RESOLVE: Cannot resolve host address: vpn.ba.mungard.de:61231 (Name or service not known)
Wed May 12 12:19:14 2021 Could not determine IPv4/IPv6 protocol
Wed May 12 12:19:14 2021 SIGUSR1[soft,init_instance] received, process restarting
Wed May 12 12:19:14 2021 Restart pause, 5 second(s)
Wed May 12 12:19:19 2021 UDP link remote: [AF_INET]195.244.254.197:61231
[...]
Wed May 12 12:19:20 2021 Initialization Sequence Completed
\end{lstlisting}

Dies wurde analog für alle Roadwarrier-Clients erfolgreich getestet.

Der Aufruf der Webseite http://www.intern.ba.mungard.de mit dem Textbrowser Lynx funktioniert (hier Azure). Somit wird immer der Server angesprochen, der am jeweiligen Cloud-Standort deployed ist:
\begin{lstlisting}[label=lynx-default-page,caption=.]
ubuntu@vpn-client-azure:~$ lynx -dump www.intern.ba.mungard.de | head -10
   Ubuntu Logo Apache2 Ubuntu Default Page
   It works! @Azure Cloud (Location: "North Europe - Dublin")

   This is the default welcome page used to test the correct operation of
   the Apache2 server after installation on Ubuntu systems. It is based on
   the equivalent page on Debian, from which the Ubuntu Apache packaging
   is derived. If you can read this page, it means that the Apache HTTP
   server installed at this site is working properly. You should replace
   this file (located at /var/www/html/index.html) before continuing to
   operate your HTTP server.

\end{lstlisting}

Damit wurden alle Kriterien für eine erfolgreiche Evaluation von Use-Case 2 erfüllt.



%dig für "next hop"n
%whois Daten